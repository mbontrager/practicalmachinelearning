---
title: "Practical Machine Learning Course Project"
author: "Martin Bontrager"
date: "April 26, 2016"
output: html_document
---

```{r load_libraries, message=FALSE}
library(caret); library(ggplot2); library(reshape2)
```


#Background

There are a host of new wearable electronic devices which can track the quantity of activity in which a person engages. An unresolved problem, however, is how to determine whether such activity is being done properly. In other words, we would like to know about the quality of activity/exercise, not just the quantity.

The goal of this project is to use data from accelerometer measurements of participants engaged in different classes of physical activity - bicep curls in this case - to predict whether the exercise is being done properly or improperly based solely on accelerometer measurements. The participants were each instructed to perform the exercise either properly (Class A) or in a way which replicated 4 common weightlifting mistakes (Classes B, C, D, and E). More information is available online [here](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises) and the results of this study have been published [here](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). 

A brief description of the study follows (from the website):


>###Weight Lifting Exercises Dataset
>
>This human activity recognition research has traditionally focused on discriminating between different activities, i.e. to predict "which" activity was performed at a specific point in time (like with the Daily Living Activities dataset above). The approach we propose for the Weight Lifting Exercises dataset is to investigate "how (well)" an activity was performed by the wearer. The "how (well)" investigation has only received little attention so far, even though it potentially provides useful information for a large variety of applications,such as sports training.
>
>In this work (see the paper) we first define quality of execution and investigate three aspects that pertain to qualitative activity recognition: the problem of specifying correct execution, the automatic and robust detection of execution mistakes, and how to provide feedback on the quality of execution to the user. We tried out an on-body sensing approach (dataset here), but also an "ambient sensing approach" (by using Microsoft Kinect - dataset still unavailable)
>
>Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).
>
>Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).


```{r import_data}
set.seed(3399)
urlTraining <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlValidation <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
fullData <- read.csv(urlTraining, header = T, na.strings = c("", "NA"))
validation <- read.csv(urlValidation, header = T, na.strings = c("", "NA"))
nzv <- nearZeroVar(fullData,saveMetrics=TRUE)
```



One of the first things to note is that there are many dummy variables in these data. Several columns do not have measurements for each observation, but are rather summary statistics for one sliding window. The paper suggests that a "sliding window" of time is anywhere between 0.5 and 2.5 seconds of activity.

In the validation data which we would like to predict are just random draws of one observation at a particular time point. That being the case, we can easily omit many of the summary variables in this dataset as they are not useful for this particular prediction problem, and also seem to be mislabeled in some cases (e.g. the summary statistics are in the wrong columns). The inappropriate structure of these variables is borne out by the near zero variances of these variables: 

```{r nzv}
nzv
```



First we can remove all rows which contain summary statistics and not observational data. We can also filter out all columns which serve only to present summary data. We can also remove the "X" variable, which is just the row number and the "new_window" variable which was only a marker for summary variable rows:



```{r clean_data, message=FALSE}
library(dplyr)
fullData <- filter(fullData, new_window == "no")
fullData <- Filter(function(x)!all(is.na(x)), fullData)
fullData <- select(fullData, -X, -new_window)

# Also remove all the same columns from the test data set:
validation <- Filter(function(x)!all(is.na(x)), validation)
validation <- select(validation, -X, -new_window)

#Have we done a good job?
nsv <- nearZeroVar(fullData, saveMetrics=TRUE)
nsv
```


#Exploring the data


**Note: I originally split this set into training and testing prior to exploring the data and removing inappropriate variables, but for the purposes of this report I show that analysis first, then split the data after all of the offending columns are removed**

While there are a lot of continuous and integer variables in these data, there is an interesting pattern that we can glean from the factor variables: most notably from the `user_name`, `classe`, and `cvtd_timestamp` variables.

In the following exploratory plot, for example, we can see that the participants in this study all performed these trials in temporal order. They all started doing biceps curls the proper way (Class A), then proceeded with Class B, then C, etc.

For the purposes of our prediction in which we're trying to assign "new" data to the proper class, the timestamp is a very useful predictor. There is also a strong predictive value to the "num_window" variable as you can see in the second plot below.


```{r explore_plot}
theme_set(theme_bw(base_size = 18))
a <- qplot(classe, cvtd_timestamp, data=fullData, color=user_name, size=I(3))
b <- qplot(classe, num_window, data=fullData, color=user_name, size=I(3))

grid.arrange(a, b, ncol=2)
```




This relationship is an artifact of the study design which would allow us to predict the validation data with great accuracy, but will fail if we are trying to accurately predict new data given only accelerometer measurements. Therefore it is necessary to further exclude all of the timestamp and username data if we want to accurately predict the exercise class based solely on accelerometer measurements.


### Dealing with multicollinearity

Now I want to examine these data for multicollinearity. I want to trim variables that are highly correlated

```{r correlation}
cor.matrix <- cor(fullData[sapply(fullData, is.numeric)])
c <- melt(cor.matrix)
qplot(x=Var1, y=Var2, data=c, fill=value, geom="tile") +
   scale_fill_gradient2(limits=c(-1, 1)) +
    theme(axis.text.x = element_text(angle=-90, vjust=0.5, hjust=0))
```


It does seem that some of the variables are highly correlated. I'll pare down the dataset by removing variable that are highly correlated (over 0.9):

```{r multicollinearity}
c <- findCorrelation(cor.matrix, cutoff = .90)
fullData <- fullData[,-c]
validation <- validation[,-c]
```


```{r exclude_time_user}
fullData <- filter(fullData, new_window == "no")
fullData <- Filter(function(x)!all(is.na(x)), fullData)
fullData <- select(fullData, -X, -new_window, -user_name, -cvtd_timestamp, 
                   -raw_timestamp_part_1, -raw_timestamp_part_2, -num_window)

# Also remove all the same columns from the test data set:
validation <- Filter(function(x)!all(is.na(x)), validation)
validation <- select(validation, -X, -new_window, -user_name, -cvtd_timestamp, 
                   -raw_timestamp_part_1, -raw_timestamp_part_2, -num_window)
```

Looking through the data using `hist` from the `Hmisc` package on the full data set, some of the variables seem normally distributed while others are clearly multimodal. This is just to show that there are some variables in which it's unwise to assume normality. Here are just a few examples:

```{r histograms, message=FALSE}
library(gridBase)
a <- qplot(total_accel_forearm, data = fullData)
b <- qplot(accel_forearm_z, data = fullData)
c <- qplot(total_accel_belt, data = fullData)
d <- qplot(accel_belt_z, data = fullData)
grid.arrange(a, b, c, d, ncol=2)
```


# Cross validation set

While the data is split online into a "training" and "testing" set already, the "testing" set is really for the purposes of validation for the assignment. I will split the full data (what is labeled as the "training" data online) into training and testing sets in order to continue these analyses. I am splitting into 70% training and 30% testing.



```{r naive_model, echo=FALSE}
inTrain <- createDataPartition(y=fullData$classe, p=0.7, list=FALSE)
training <- fullData[inTrain, ]; testing <- fullData[-inTrain, ]
```




# Model Exploration and Building


To begin with, I will try to build a model using classification trees. It's somewhat unclear to me based on what I've learned so far in the class which model building method is most appropriate in certain situations. I need to learn a lot more about that on my own, however it seems like it might be best to build models without the assumptions of normality since that assumption is violated in some of these variables. 

Here is a classification tree:

```{r class_tree, message=FALSE}
library(rattle)
modfit_1 <- train(classe ~ ., method="rpart", data=training)
fancyRpartPlot(modfit_1$finalModel)
```



